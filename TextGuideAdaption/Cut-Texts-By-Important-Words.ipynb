{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-22T18:05:18.675295Z",
     "iopub.status.busy": "2023-10-22T18:05:18.673822Z",
     "iopub.status.idle": "2023-10-22T18:07:22.408320Z",
     "shell.execute_reply": "2023-10-22T18:07:22.407128Z",
     "shell.execute_reply.started": "2023-10-22T18:05:18.675262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-ddba235724ea2167/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb36dc54ede4aff9b102fd2fa36bcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8638ac6f84b1443987b7a3f5bb0d1c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-ddba235724ea2167/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n",
      "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-cb1306363117a2b2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6786fa0db01474d9a5b8e47a1f20b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590f3dbff77440cabd3c971c5b25717b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-cb1306363117a2b2/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_id</th>\n",
       "      <th>text</th>\n",
       "      <th>pornographic-content</th>\n",
       "      <th>violence</th>\n",
       "      <th>death</th>\n",
       "      <th>sexual-assault</th>\n",
       "      <th>abuse</th>\n",
       "      <th>blood</th>\n",
       "      <th>suicide</th>\n",
       "      <th>pregnancy</th>\n",
       "      <th>...</th>\n",
       "      <th>sexism</th>\n",
       "      <th>miscarriages</th>\n",
       "      <th>transphobia</th>\n",
       "      <th>abortion</th>\n",
       "      <th>fat-phobia</th>\n",
       "      <th>animal-death</th>\n",
       "      <th>ableism</th>\n",
       "      <th>classism</th>\n",
       "      <th>misogyny</th>\n",
       "      <th>animal-cruelty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23758279</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p&gt;\\n  &lt;em&gt;“A little le...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23780866</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;hr&gt;&lt;p align=\"center\"&gt;&amp;...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23754256</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;hr&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;\"Ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23757385</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p&gt;It was near midnight...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23771095</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p class=\"western\"&gt;&amp;nbs...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307097</th>\n",
       "      <td>19181050</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p&gt;The first time Jin h...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307098</th>\n",
       "      <td>19179817</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p&gt;Ah, love. It was som...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307099</th>\n",
       "      <td>19197943</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p&gt;Clarke wakes up earl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307100</th>\n",
       "      <td>19164955</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p&gt;Sergio felt tense wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307101</th>\n",
       "      <td>19174132</td>\n",
       "      <td>&lt;div class=\"userstuff\"&gt;&lt;p&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;\\n  &lt;sp...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307102 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         work_id                                               text  \\\n",
       "0       23758279  <div class=\"userstuff\"><p>\\n  <em>“A little le...   \n",
       "1       23780866  <div class=\"userstuff\"><hr><p align=\"center\">&...   \n",
       "2       23754256  <div class=\"userstuff\"><hr><p>&nbsp;</p><p>\"Ba...   \n",
       "3       23757385  <div class=\"userstuff\"><p>It was near midnight...   \n",
       "4       23771095  <div class=\"userstuff\"><p class=\"western\">&nbs...   \n",
       "...          ...                                                ...   \n",
       "307097  19181050  <div class=\"userstuff\"><p>The first time Jin h...   \n",
       "307098  19179817  <div class=\"userstuff\"><p>Ah, love. It was som...   \n",
       "307099  19197943  <div class=\"userstuff\"><p>Clarke wakes up earl...   \n",
       "307100  19164955  <div class=\"userstuff\"><p>Sergio felt tense wh...   \n",
       "307101  19174132  <div class=\"userstuff\"><p>&nbsp;</p><p>\\n  <sp...   \n",
       "\n",
       "        pornographic-content  violence  death  sexual-assault  abuse  blood  \\\n",
       "0                          1         0      0               0      0      0   \n",
       "1                          1         0      0               0      0      0   \n",
       "2                          1         0      0               0      0      0   \n",
       "3                          0         0      0               0      0      0   \n",
       "4                          1         0      0               0      0      0   \n",
       "...                      ...       ...    ...             ...    ...    ...   \n",
       "307097                     1         0      0               0      0      0   \n",
       "307098                     1         0      0               0      0      0   \n",
       "307099                     0         0      0               0      0      0   \n",
       "307100                     1         0      0               1      0      0   \n",
       "307101                     1         0      0               0      0      0   \n",
       "\n",
       "        suicide  pregnancy  ...  sexism  miscarriages  transphobia  abortion  \\\n",
       "0             0          0  ...       0             0            0         0   \n",
       "1             0          0  ...       0             0            0         0   \n",
       "2             0          0  ...       0             0            0         0   \n",
       "3             1          0  ...       0             0            0         0   \n",
       "4             0          0  ...       0             0            0         0   \n",
       "...         ...        ...  ...     ...           ...          ...       ...   \n",
       "307097        0          0  ...       0             0            0         0   \n",
       "307098        0          0  ...       0             0            0         0   \n",
       "307099        0          1  ...       0             0            0         0   \n",
       "307100        0          0  ...       0             0            0         0   \n",
       "307101        0          0  ...       0             0            0         0   \n",
       "\n",
       "        fat-phobia  animal-death  ableism  classism  misogyny  animal-cruelty  \n",
       "0                0             0        0         0         0               0  \n",
       "1                0             0        0         0         0               0  \n",
       "2                0             0        0         0         0               0  \n",
       "3                0             0        0         0         0               0  \n",
       "4                0             0        0         0         0               0  \n",
       "...            ...           ...      ...       ...       ...             ...  \n",
       "307097           0             0        0         0         0               0  \n",
       "307098           0             0        0         0         0               0  \n",
       "307099           0             0        0         0         0               0  \n",
       "307100           0             0        0         0         0               0  \n",
       "307101           0             0        0         0         0               0  \n",
       "\n",
       "[307102 rows x 34 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "label_columns = ['pornographic-content', 'violence', 'death', 'sexual-assault', 'abuse', 'blood',\n",
    "                 'suicide', 'pregnancy', 'child-abuse', 'incest', 'underage', 'homophobia',\n",
    "                 'self-harm', 'dying', 'kidnapping', 'mental-illness', 'dissection',\n",
    "                 'eating-disorders', 'abduction', 'body-hatred', 'childbirth', 'racism',\n",
    "                 'sexism', 'miscarriages', 'transphobia', 'abortion', 'fat-phobia',\n",
    "                 'animal-death', 'ableism', 'classism', 'misogyny', 'animal-cruelty']\n",
    "\n",
    "train = load_dataset(\"json\", \n",
    "                     data_files=\"/kaggle/input/pan23trigger/pan23-trigger-detection/pan23-trigger-detection-train/works.jsonl\", \n",
    "                     streaming=False, split=\"train\")\n",
    "\n",
    "val = load_dataset(\"json\", \n",
    "                     data_files=\"/kaggle/input/pan23trigger/pan23-trigger-detection/pan23-trigger-detection-validation/works.jsonl\", \n",
    "                     streaming=False, split=\"train\")\n",
    "\n",
    "def to_df(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # One-hot encode the \"labels\" column\n",
    "    onehot_labels = pd.DataFrame(np.array(df['labels'].tolist()), columns=label_columns)\n",
    "\n",
    "    # Remove the original \"labels\" column from the DataFrame\n",
    "    df.drop(columns=['labels'], inplace=True)\n",
    "\n",
    "    # Concatenate the one-hot encoded labels with the original DataFrame\n",
    "    encoded_df = pd.concat([df, onehot_labels], axis=1)\n",
    "\n",
    "    return encoded_df\n",
    "\n",
    "train = to_df(train)\n",
    "val = to_df(val)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_class_combinations(df, label_columns):\n",
    "    # Get the class combinations for each data entry\n",
    "    class_combinations = df[label_columns].apply(tuple, axis=1)\n",
    "    combination_counts = Counter(class_combinations)\n",
    "    \n",
    "    # Print the top 15 most common classes and their counts before undersampling\n",
    "    most_common_classes_before = combination_counts.most_common(15)\n",
    "    print(\"Top 15 Most Common Classes Before Undersampling:\")\n",
    "    for combination, count in most_common_classes_before:\n",
    "        print(f\"Class Combination: {combination}, Count: {count}\")\n",
    "\n",
    "    # Plot class combinations before undersampling\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(range(len(combination_counts)), list(combination_counts.values()))\n",
    "    plt.title('Class Combinations Before Undersampling')\n",
    "    plt.ylabel('Count (log scale)')\n",
    "    plt.yscale('log') #symlog\n",
    "    plt.xticks([])\n",
    "\n",
    "    # Sort class combinations by count in descending order\n",
    "    sorted_combinations = sorted(combination_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate the maximum allowed count for the majority class combinations\n",
    "    max_count = sorted_combinations[0][1]\n",
    "    second_max_count = sorted_combinations[1][1]\n",
    "    max_count_limit = int(second_max_count * 5)\n",
    "\n",
    "    # Create a dictionary to map class combinations to indices\n",
    "    combination_indices = {}\n",
    "    for i, cc in enumerate(class_combinations):\n",
    "        if cc not in combination_indices:\n",
    "            combination_indices[cc] = [i]\n",
    "        else:\n",
    "            combination_indices[cc].append(i)\n",
    "\n",
    "    # Undersample the majority class combinations\n",
    "    undersampled_indices = [random.sample(indices, max_count_limit) for _, indices in tqdm(combination_indices.items()) if len(indices) > max_count_limit]\n",
    "    undersampled_indices = [idx for indices in undersampled_indices for idx in indices]\n",
    "\n",
    "    # Create the balanced DataFrame\n",
    "    df_balanced = df.iloc[undersampled_indices].copy()\n",
    "    df_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Include entries from other classes\n",
    "    for _, indices in tqdm(combination_indices.items()):\n",
    "        if len(indices) <= max_count_limit:\n",
    "            df_balanced = pd.concat([df_balanced, df.iloc[indices]])\n",
    "\n",
    "    # Update the counts of class combinations after undersampling\n",
    "    class_combinations_balanced = df_balanced[label_columns].apply(tuple, axis=1)\n",
    "    combination_counts_balanced = Counter(class_combinations_balanced)\n",
    "\n",
    "    # Plot class combinations after undersampling\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(range(len(combination_counts_balanced)), list(combination_counts_balanced.values()))\n",
    "    plt.title('Class Combinations After Undersampling')\n",
    "    plt.xlabel('Class Combinations')\n",
    "    plt.ylabel('Count (log scale)')\n",
    "    plt.yscale('log') #symlog\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the top 15 most common classes and their counts after undersampling\n",
    "    most_common_classes_after = combination_counts_balanced.most_common(15)\n",
    "    print(\"\\nTop 15 Most Common Classes After Undersampling:\")\n",
    "    for combination, count in most_common_classes_after:\n",
    "        print(f\"Class Combination: {combination}, Count: {count}\")\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "#df = df.head(10000)\n",
    "#train = undersample_class_combinations(train, label_columns)\n",
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T20:47:18.040911Z",
     "iopub.status.busy": "2023-10-11T20:47:18.039126Z",
     "iopub.status.idle": "2023-10-11T20:47:18.060508Z",
     "shell.execute_reply": "2023-10-11T20:47:18.059429Z",
     "shell.execute_reply.started": "2023-10-11T20:47:18.040868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['humiliation',\n",
       " 'stepfather',\n",
       " 'raped',\n",
       " 'underage',\n",
       " 'autistic',\n",
       " 'eat',\n",
       " 'transgender',\n",
       " 'retard',\n",
       " 'money',\n",
       " 'bitch',\n",
       " 'racist',\n",
       " 'belly',\n",
       " 'gay',\n",
       " 'asexual',\n",
       " 'dysphoria',\n",
       " 'torture',\n",
       " 'grief',\n",
       " 'throat',\n",
       " 'kidnapping',\n",
       " 'abduction',\n",
       " 'blood',\n",
       " 'surgery',\n",
       " 'cutting',\n",
       " 'suicide',\n",
       " 'omega',\n",
       " 'incest',\n",
       " 'contraction',\n",
       " 'abortion',\n",
       " 'miscarriage',\n",
       " 'belly',\n",
       " 'lan',\n",
       " 'king']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/kaggle/input/most-important-words/most_important_words.json\", 'r') as json_file:\n",
    "    loaded_list = json.load(json_file)\n",
    "\n",
    "loaded_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T20:47:18.062830Z",
     "iopub.status.busy": "2023-10-11T20:47:18.062110Z",
     "iopub.status.idle": "2023-10-11T20:47:33.811693Z",
     "shell.execute_reply": "2023-10-11T20:47:33.810964Z",
     "shell.execute_reply.started": "2023-10-11T20:47:18.062799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e5e19ea28e407c8fefdb8937495631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5ed38cbd0e4156b0bd3bbc9ea841f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df573ce826d4d64ba59b3e6e4a4bf49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/846k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa038c38f2e4664b2a9766f1da704c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/775 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "humiliation ['▁humiliation']\n",
      "stepfather ['▁step', 'father']\n",
      "raped ['▁raped']\n",
      "underage ['▁underage']\n",
      "autistic ['▁autistic']\n",
      "eat ['▁eat']\n",
      "transgender ['▁transgender']\n",
      "retard ['▁retard']\n",
      "money ['▁money']\n",
      "bitch ['▁bitch']\n",
      "racist ['▁racist']\n",
      "belly ['▁belly']\n",
      "gay ['▁gay']\n",
      "asexual ['▁a', 'sexual']\n",
      "dysphoria ['▁dysph', 'oria']\n",
      "torture ['▁torture']\n",
      "grief ['▁grief']\n",
      "throat ['▁throat']\n",
      "kidnapping ['▁kidnapping']\n",
      "abduction ['▁abduction']\n",
      "blood ['▁blood']\n",
      "surgery ['▁surgery']\n",
      "cutting ['▁cutting']\n",
      "suicide ['▁suicide']\n",
      "omega ['▁omega']\n",
      "incest ['▁incest']\n",
      "contraction ['▁contraction']\n",
      "abortion ['▁abortion']\n",
      "miscarriage ['▁miscarriage']\n",
      "belly ['▁belly']\n",
      "lan ['▁lan']\n",
      "king ['▁king']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, AutoTokenizer\n",
    "\n",
    "MAX_LEN = 510\n",
    "#MAX_LEN = 4096\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained('google/bigbird-roberta-base', max_length=MAX_LEN)\n",
    "\n",
    "for word in loaded_list[0]:\n",
    "    print(word, tokenizer.tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T20:47:33.813579Z",
     "iopub.status.busy": "2023-10-11T20:47:33.812872Z",
     "iopub.status.idle": "2023-10-11T20:47:33.820758Z",
     "shell.execute_reply": "2023-10-11T20:47:33.819712Z",
     "shell.execute_reply.started": "2023-10-11T20:47:33.813550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁general', '-', 'purpose', '▁architectures', '▁', 'bert', '▁g', 'pt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"general-purpose architectures bert gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T20:47:33.823032Z",
     "iopub.status.busy": "2023-10-11T20:47:33.822724Z",
     "iopub.status.idle": "2023-10-11T20:47:34.180200Z",
     "shell.execute_reply": "2023-10-11T20:47:34.179114Z",
     "shell.execute_reply.started": "2023-10-11T20:47:33.823007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁humiliation',\n",
       "  '▁step',\n",
       "  'father',\n",
       "  '▁raped',\n",
       "  '▁underage',\n",
       "  '▁autistic',\n",
       "  '▁eat',\n",
       "  '▁transgender',\n",
       "  '▁retard',\n",
       "  '▁money',\n",
       "  '▁bitch',\n",
       "  '▁racist',\n",
       "  '▁belly',\n",
       "  '▁gay',\n",
       "  '▁a',\n",
       "  'sexual',\n",
       "  '▁dysph',\n",
       "  'oria',\n",
       "  '▁torture',\n",
       "  '▁grief',\n",
       "  '▁throat',\n",
       "  '▁kidnapping',\n",
       "  '▁abduction',\n",
       "  '▁blood',\n",
       "  '▁surgery',\n",
       "  '▁cutting',\n",
       "  '▁suicide',\n",
       "  '▁omega',\n",
       "  '▁incest',\n",
       "  '▁contraction',\n",
       "  '▁abortion',\n",
       "  '▁miscarriage',\n",
       "  '▁belly',\n",
       "  '▁lan',\n",
       "  '▁king']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_list = []\n",
    "\n",
    "for sublist in loaded_list:\n",
    "    tokenized_sublist = []\n",
    "    for word in sublist:\n",
    "        tokenzied_word = tokenizer.tokenize(word)\n",
    "        tokenized_sublist.append(tokenzied_word)\n",
    "    tokenized_sublist = [item for sublist in tokenized_sublist for item in sublist] # flatten\n",
    "    tokenized_list.append(tokenized_sublist)\n",
    "\n",
    "tokenized_list[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T20:52:25.224451Z",
     "iopub.status.busy": "2023-10-11T20:52:25.223918Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 37998/307102 [39:45<7:36:16,  9.83it/s] "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "END_TOKENS_LEN = 100\n",
    "TOKEN_NEIGHBOURS = 8 #4\n",
    "\n",
    "def preprocess_function(example):\n",
    "    #Remove irrelevant data and special character:\n",
    "    example[\"text\"] = BeautifulSoup(example[\"text\"], \"html.parser\").get_text(strip=True) #remove links\n",
    "    example[\"text\"] = re.sub(r'\\xa0', ' ', str(example[\"text\"])) # remove further html stuff\n",
    "    example[\"text\"] = re.sub(r'\\n', ' ', str(example[\"text\"])) # remove paragraphs\n",
    "    example[\"text\"] = re.sub(r'\\s+', ' ', str(example[\"text\"])) # multiple spaces to one space\n",
    "    example[\"text\"] = re.sub(r'[^\\x00-\\x7F]+', '', str(example[\"text\"])) #remove non asci\n",
    "        \n",
    "    return example\n",
    "\n",
    "def pad(text, max_sequence_length, token):\n",
    "    num_padding = max_sequence_length - len(text)\n",
    "\n",
    "    # Create a padding mask\n",
    "    padding_mask = np.zeros(max_sequence_length, dtype=int)\n",
    "\n",
    "    # Pad the list with zeros to the maximum length\n",
    "    padded_list = np.pad(text, (0, num_padding), mode='constant', constant_values=token)\n",
    "\n",
    "    # Update the padding mask to indicate the padded positions\n",
    "    padding_mask[:len(text)] = 1\n",
    "      \n",
    "    return padded_list, padding_mask\n",
    "\n",
    "def extract_subtext(data):\n",
    "    data = preprocess_function(data)\n",
    "    \n",
    "    input_tokens = tokenizer.tokenize(data[\"text\"])\n",
    "    \n",
    "    final_text = []\n",
    "    token_count = 0\n",
    "    \n",
    "    if len(input_tokens) > MAX_LEN:\n",
    "        end_of_text = input_tokens[-END_TOKENS_LEN:]\n",
    "        del input_tokens[-END_TOKENS_LEN:]\n",
    "        \n",
    "        for ranking, important_words in enumerate(tokenized_list):\n",
    "            for i, word in enumerate(input_tokens):\n",
    "                if word in important_words:\n",
    "                    start_index = max(0, i - TOKEN_NEIGHBOURS)\n",
    "                    end_index = min(len(input_tokens), i + (TOKEN_NEIGHBOURS + 1))\n",
    "\n",
    "                    chosen_tokens = input_tokens[start_index:end_index]\n",
    "                    final_text.extend(chosen_tokens)\n",
    "\n",
    "                    del input_tokens[start_index:end_index]\n",
    "\n",
    "                    token_count += len(chosen_tokens)\n",
    "\n",
    "                if token_count >= MAX_LEN - END_TOKENS_LEN:\n",
    "                    break\n",
    "            if token_count >= MAX_LEN - END_TOKENS_LEN:\n",
    "                break\n",
    "    \n",
    "        if len(final_text) > MAX_LEN - END_TOKENS_LEN:\n",
    "            final_text = final_text[:MAX_LEN - END_TOKENS_LEN]\n",
    "\n",
    "        token_count = token_count + len(end_of_text)\n",
    "        final_text = final_text + end_of_text\n",
    "        \n",
    "        final_text = final_text[:MAX_LEN] \n",
    "    else:\n",
    "        final_text = input_tokens\n",
    "        \n",
    "    final_text[0] = tokenizer.cls_token\n",
    "    final_text[-1] = tokenizer.sep_token\n",
    "    \n",
    "    final_text, mask = pad(final_text, MAX_LEN, tokenizer.pad_token)\n",
    "    final_tokens = tokenizer.convert_tokens_to_ids(final_text)\n",
    "    \n",
    "    #data[\"tokens\"] = final_tokens\n",
    "    data[\"text\"] = final_tokens\n",
    "    data[\"mask\"] = mask\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "train = train.progress_apply(lambda x: extract_subtext(x), axis=1)\n",
    "val = val.progress_apply(lambda x: extract_subtext(x), axis=1)\n",
    "#train.iloc[1:2].progress_apply(lambda x: extract_subtext(x), axis=1)[[\"mask\", \"text\"]]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_path = 'train_cutted_tokenized.pkl'\n",
    "val_output_path = 'val_cutted_tokenized.pkl'\n",
    "\n",
    "# Use the to_pickle method to save the DataFrame to a pickle file\n",
    "train.to_pickle(train_output_path)\n",
    "val.to_pickle(val_output_path)\n",
    "\n",
    "# Confirm that the file has been saved\n",
    "print(f\"DataFrame saved to {train_output_path} and {val_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3125670,
     "sourceId": 5587659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3786772,
     "sourceId": 6587993,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
