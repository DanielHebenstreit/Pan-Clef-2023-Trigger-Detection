{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file:///C:/Users/danih/Downloads/A%201012782908347.pdf Hierarchical Text CategorizationUsing Neural Networks MIGUEL E. RUIZ\n",
    "\n",
    "\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import pandas as pd \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import text\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")\n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['pornographic-content', 'violence', 'death', 'sexual-assault', 'abuse', 'blood',\n",
    "                 'suicide', 'pregnancy', 'child-abuse', 'incest', 'underage', 'homophobia',\n",
    "                 'self-harm', 'dying', 'kidnapping', 'mental-illness', 'dissection',\n",
    "                 'eating-disorders', 'abduction', 'body-hatred', 'childbirth', 'racism',\n",
    "                 'sexism', 'miscarriages', 'transphobia', 'abortion', 'fat-phobia',\n",
    "                 'animal-death', 'ableism', 'classism', 'misogyny', 'animal-cruelty']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Document for Document:\n",
    "# - html code\n",
    "# - special characters\n",
    "# - multiple spaces\n",
    "# - lowercasing\n",
    "# - non alphabetic characters\n",
    "# - non english words\n",
    "# - lemmatization\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "english_words = set(wordnet.words())\n",
    "\n",
    "# Define the label columns in the desired order\n",
    "\n",
    "CREATE_DATA = False\n",
    "\n",
    "def preprocess_function(text):\n",
    "    # Remove irrelevant data and special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(strip=True)\n",
    "    text = re.sub(r'\\xa0', ' ', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r'(\\w)\\1{2,}', '', text) # Remove words with the same letter repeated four times or more\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Preprocess each word using list comprehensions\n",
    "    words = [re.sub(r'[^a-zA-Z]', '', word) for word in words]  # Remove non-alphabetic characters\n",
    "\n",
    "    # Filter out non-English words\n",
    "    words = [word for word in words if word in english_words]\n",
    "\n",
    "    # Perform POS tagging for lemmatization\n",
    "    #tagged_words = pos_tag(words)\n",
    "\n",
    "    # Lemmatize the word based on its POS tag\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #words = [lemmatizer.lemmatize(word, pos=get_wordnet_pos(pos_tag))\n",
    "    #         for word, pos_tag in tagged_words if word]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match\n",
    "\n",
    "def clean_data(path):\n",
    "    # Initialize empty lists to store the extracted data\n",
    "    contexts = []\n",
    "    work_ids = []\n",
    "    label_values = {col: [] for col in label_columns}\n",
    "\n",
    "    # Read the JSONL file and process the data\n",
    "    with open(path, 'r') as file:\n",
    "        for line_num, line in enumerate(tqdm(file)):\n",
    "            data = json.loads(line)\n",
    "            work_ids.append(data[\"work_id\"])\n",
    "            contexts.append(data[\"text\"])\n",
    "\n",
    "            for i, label_col in enumerate(label_columns):\n",
    "                label_values[label_col].append(data['labels'][i])     \n",
    "\n",
    "            #if line_num == 1000:\n",
    "            #    break\n",
    "\n",
    "    # Preprocess the text data in parallel using all available CPU cores\n",
    "    with tqdm(total=len(contexts), desc=\"Preprocessing\") as pbar:\n",
    "        with ProcessPoolExecutor() as executor:\n",
    "            contexts = list(executor.map(preprocess_function, contexts))\n",
    "            pbar.update(len(contexts))\n",
    "\n",
    "    # Create a dictionary with the extracted data\n",
    "    data_dict = {'text': contexts, 'work_id': work_ids}\n",
    "    data_dict.update(label_values)\n",
    "\n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    #df.to_csv('cleaned_data.csv', index=False)\n",
    "    #df = pd.read_csv('cleaned_data.csv')\n",
    "    \n",
    "    return df\n",
    "\n",
    "if CREATE_DATA:\n",
    "    train = clean_data(\"/kaggle/input/pan23trigger/pan23-trigger-detection/pan23-trigger-detection-train/works.jsonl\")\n",
    "    val = clean_data(\"/kaggle/input/pan23trigger/pan23-trigger-detection/pan23-trigger-detection-validation/works.jsonl\")\n",
    "else:\n",
    "    train = pd.read_csv('/kaggle/input/tf-idf-valtrain-nolemmas-cleaned/train_cleaned_data.csv')    \n",
    "    val = pd.read_csv('/kaggle/input/tf-idf-valtrain-nolemmas-cleaned/val_cleaned_data.csv')\n",
    "    \n",
    "train.to_csv('train_cleaned_data.csv', index=False)\n",
    "val.to_csv('val_cleaned_data.csv', index=False)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NA Filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[train[\"text\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna()\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data balancing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def undersample_class_combinations(df, label_columns):\n",
    "    # Get the class combinations for each data entry\n",
    "    class_combinations = df[label_columns].apply(tuple, axis=1)\n",
    "    combination_counts = Counter(class_combinations)\n",
    "    \n",
    "    # Print the top 15 most common classes and their counts before undersampling\n",
    "    most_common_classes_before = combination_counts.most_common(15)\n",
    "    print(\"Top 15 Most Common Classes Before Undersampling:\")\n",
    "    for combination, count in most_common_classes_before:\n",
    "        print(f\"Class Combination: {combination}, Count: {count}\")\n",
    "\n",
    "    # Plot class combinations before undersampling\n",
    "    plt.figure(figsize=(10, 12))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(range(len(combination_counts)), list(combination_counts.values()))\n",
    "    plt.title('Class Combinations Before Undersampling')\n",
    "    plt.ylabel('Count (log scale)')\n",
    "    plt.yscale('log') #symlog\n",
    "    plt.xticks([])\n",
    "\n",
    "    # Sort class combinations by count in descending order\n",
    "    sorted_combinations = sorted(combination_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Calculate the maximum allowed count for the majority class combinations\n",
    "    max_count = sorted_combinations[0][1]\n",
    "    second_max_count = sorted_combinations[1][1]\n",
    "    max_count_limit = int(second_max_count * 2)\n",
    "\n",
    "    # Create a dictionary to map class combinations to indices\n",
    "    combination_indices = {}\n",
    "    for i, cc in enumerate(class_combinations):\n",
    "        if cc not in combination_indices:\n",
    "            combination_indices[cc] = [i]\n",
    "        else:\n",
    "            combination_indices[cc].append(i)\n",
    "\n",
    "    # Undersample the majority class combinations\n",
    "    undersampled_indices = [random.sample(indices, max_count_limit) for _, indices in tqdm(combination_indices.items()) if len(indices) > max_count_limit]\n",
    "    undersampled_indices = [idx for indices in undersampled_indices for idx in indices]\n",
    "\n",
    "    # Create the balanced DataFrame\n",
    "    df_balanced = df.iloc[undersampled_indices].copy()\n",
    "    df_balanced.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Include entries from other classes\n",
    "    for _, indices in tqdm(combination_indices.items()):\n",
    "        if len(indices) <= max_count_limit:\n",
    "            df_balanced = pd.concat([df_balanced, df.iloc[indices]])\n",
    "\n",
    "    # Update the counts of class combinations after undersampling\n",
    "    class_combinations_balanced = df_balanced[label_columns].apply(tuple, axis=1)\n",
    "    combination_counts_balanced = Counter(class_combinations_balanced)\n",
    "\n",
    "    # Plot class combinations after undersampling\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.bar(range(len(combination_counts_balanced)), list(combination_counts_balanced.values()))\n",
    "    plt.title('Class Combinations After Undersampling')\n",
    "    plt.xlabel('Class Combinations')\n",
    "    plt.ylabel('Count (log scale)')\n",
    "    plt.yscale('log') #symlog\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the top 15 most common classes and their counts after undersampling\n",
    "    most_common_classes_after = combination_counts_balanced.most_common(15)\n",
    "    print(\"\\nTop 15 Most Common Classes After Undersampling:\")\n",
    "    for combination, count in most_common_classes_after:\n",
    "        print(f\"Class Combination: {combination}, Count: {count}\")\n",
    "\n",
    "    return df_balanced\n",
    "\n",
    "#train = train.head(100000)\n",
    "#train = undersample_class_combinations(train, label_columns)\n",
    "#train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_abuse = [\"abuse\", \"child-abuse\", 'sexual-assault', 'underage']\n",
    "g_discrimination = ['mental-illness', 'eating-disorders', 'transphobia', 'ableism', 'classism', 'misogyny', 'racism', \n",
    "                  'fat-phobia', 'homophobia', 'sexism', 'body-hatred']\n",
    "g_aggresion = [\"violence\", 'death', 'dying', 'kidnapping', 'abduction']\n",
    "g_medical = ['blood', \"dissection\"]\n",
    "g_mental = ['self-harm', 'suicide']\n",
    "g_sexual = ['pornographic-content', 'incest']\n",
    "g_pregnancy = ['childbirth', \"abortion\", 'miscarriages', 'pregnancy']\n",
    "g_animal = ['animal-death', 'animal-cruelty']\n",
    "\n",
    "coarse_grained = [\"g_abuse\", \"g_discrimination\", \"g_aggresion\", \"g_medical\", \"g_mental\", \"g_sexual\", \"g_pregnancy\", \"g_animal\"]\n",
    "coarse_grained_arrays = [g_abuse, g_discrimination, g_aggresion, g_medical, g_mental, g_sexual, g_pregnancy, g_animal]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"g_abuse\"] = (train[g_abuse] == 1).any(axis=1).astype(int)\n",
    "train[\"g_discrimination\"] = (train[g_discrimination] == 1).any(axis=1).astype(int)\n",
    "train[\"g_aggresion\"] = (train[g_aggresion] == 1).any(axis=1).astype(int)\n",
    "train[\"g_medical\"] = (train[g_medical] == 1).any(axis=1).astype(int)\n",
    "train[\"g_mental\"] = (train[g_mental] == 1).any(axis=1).astype(int)\n",
    "train[\"g_sexual\"] = (train[g_sexual] == 1).any(axis=1).astype(int)\n",
    "train[\"g_pregnancy\"] = (train[g_pregnancy] == 1).any(axis=1).astype(int)\n",
    "train[\"g_animal\"] = (train[g_animal] == 1).any(axis=1).astype(int)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, y_train = train[['text', 'work_id']], train[label_columns + coarse_grained]\n",
    "X_val, y_val = val[['text', 'work_id']], val[label_columns]\n",
    "\n",
    "del train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for label in label_columns:\n",
    "    weight = len(y_train[\"pornographic-content\"]) / y_train[label].sum()\n",
    "    weights.append(weight)\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True, ngram_range=(1,1), max_features=50000)\n",
    "\n",
    "tfidf_df = vectorizer.fit_transform(X_train[\"text\"])\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "test_vec = vectorizer.transform(X_val[\"text\"])\n",
    "test_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_grained_model = MultiOutputClassifier(LinearSVC()).fit(tfidf_df, y_train[coarse_grained])\n",
    "\n",
    "coarse_grained_pred = coarse_grained_model.predict(test_vec)\n",
    "coarse_grained_pred = pd.DataFrame(coarse_grained_pred, columns=coarse_grained)\n",
    "coarse_grained_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_grained = [\"g_abuse\", \"g_discrimination\", \"g_aggresion\", \"g_medical\", \"g_mental\", \"g_sexual\", \"g_pregnancy\", \"g_animal\"]\n",
    "coarse_grained_arrays = [g_abuse, g_discrimination, g_aggresion, g_medical, g_mental, g_sexual, g_pregnancy, g_animal]\n",
    "\n",
    "for column, arrays in zip(coarse_grained, coarse_grained_arrays):\n",
    "    print(\"In validation set coarse\", column, \"-\", (y_val[arrays] == 1).any(axis=1).astype(int).sum(), \", predicted:\", coarse_grained_pred[column].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_grained_models = []\n",
    "\n",
    "for column, arrays in zip(coarse_grained, coarse_grained_arrays):\n",
    "    mask = y_train[column] == 1\n",
    "    model = MultiOutputClassifier(LinearSVC()).fit(tfidf_df[mask], y_train[mask][arrays])\n",
    "\n",
    "    prediction_mask = coarse_grained_pred[column] == 1\n",
    "    pred = model.predict(test_vec[prediction_mask])\n",
    "    \n",
    "    X_val[arrays] = 0\n",
    "    X_val.loc[prediction_mask, arrays] = pred\n",
    "    \n",
    "    fine_grained_models.append(model)\n",
    "    \n",
    "    print(column, \", train data: \", len(y_train[mask]))\n",
    "    for f_col in arrays:\n",
    "        print(\"   >>>> \", f_col, \"predicted\", X_val[f_col].sum(), \"total\", y_val[f_col].sum())\n",
    "        \n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_val, X_val[label_columns]))\n",
    "print(f1_score(y_val, X_val[label_columns], average='macro', zero_division=1))\n",
    "print(f1_score(y_val, X_val[label_columns], average='micro', zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = []\n",
    "n_of_top_words = 250\n",
    "\n",
    "for m in fine_grained_models:\n",
    "    for e in m.estimators_:\n",
    "        estimators.append(e)\n",
    "\n",
    "flattened_fine_grained_labels = [item for sublist in coarse_grained_arrays for item in sublist]\n",
    "most_important_words_list = [[] for _ in range(n_of_top_words)]\n",
    "\n",
    "for name, estimator in zip(flattened_fine_grained_labels, estimators):    \n",
    "    top_words = np.argsort(estimator.coef_[0])[::-1][0:n_of_top_words]\n",
    "    \n",
    "    print(name)\n",
    "    \n",
    "    for idx, value in enumerate(top_words):\n",
    "        most_important_words_list[idx].append(feature_names[value])\n",
    "        \n",
    "        if idx < 10:\n",
    "            print(\"  >>\", feature_names[value])\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('most_important_words.json', 'w') as json_file:\n",
    "    json.dump(most_important_words_list, json_file)\n",
    "\n",
    "# Reading the list of lists from the JSON file\n",
    "with open('most_important_words.json', 'r') as json_file:\n",
    "    loaded_list = json.load(json_file)\n",
    "\n",
    "loaded_list"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3125670,
     "sourceId": 5587659,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3411670,
     "sourceId": 5961073,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3741864,
     "sourceId": 6477264,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3785314,
     "sourceId": 6549445,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30558,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
